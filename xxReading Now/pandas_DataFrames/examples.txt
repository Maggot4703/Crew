# pandas DataFrames - Examples and Code Samples

## Example 1: DataFrame Creation and Basic Operations
```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Creating DataFrames from different sources
# From dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [25, 30, 35, 28, 32],
    'City': ['New York', 'London', 'Tokyo', 'Paris', 'Sydney'],
    'Salary': [70000, 85000, 95000, 72000, 88000],
    'Department': ['Engineering', 'Marketing', 'Engineering', 'HR', 'Finance']
}
df = pd.DataFrame(data)
print("Basic DataFrame:")
print(df)

# From lists
employees = [
    ['Alice', 25, 'New York', 70000],
    ['Bob', 30, 'London', 85000],
    ['Charlie', 35, 'Tokyo', 95000]
]
df_from_lists = pd.DataFrame(employees, columns=['Name', 'Age', 'City', 'Salary'])

# From numpy array
np_data = np.random.randn(5, 4)
df_from_numpy = pd.DataFrame(np_data, columns=['A', 'B', 'C', 'D'])

# Creating with date index
dates = pd.date_range('2024-01-01', periods=10, freq='D')
df_with_dates = pd.DataFrame({
    'Sales': np.random.randint(100, 1000, 10),
    'Customers': np.random.randint(10, 100, 10)
}, index=dates)

# Basic DataFrame information
print(f"\nDataFrame shape: {df.shape}")
print(f"Column names: {df.columns.tolist()}")
print(f"Data types:\n{df.dtypes}")
print(f"Memory usage:\n{df.memory_usage(deep=True)}")

# Basic statistics
print(f"\nBasic statistics:\n{df.describe()}")
print(f"\nInfo about DataFrame:")
df.info()

# Missing data handling
# Create DataFrame with missing values
data_with_na = {
    'A': [1, 2, np.nan, 4, 5],
    'B': [1, np.nan, 3, np.nan, 5],
    'C': ['x', 'y', 'z', np.nan, 'w']
}
df_na = pd.DataFrame(data_with_na)
print(f"\nMissing values:\n{df_na.isnull().sum()}")

# Fill missing values
df_filled = df_na.fillna({'A': df_na['A'].mean(), 'B': df_na['B'].median(), 'C': 'unknown'})
print(f"\nAfter filling missing values:\n{df_filled}")
```

## Example 2: Data Selection and Filtering
```python
# Various ways to select data
# Single column
names = df['Name']
print("Names column:")
print(names)

# Multiple columns
subset = df[['Name', 'Salary', 'Department']]
print("\nSubset of columns:")
print(subset)

# Row selection by index
first_row = df.iloc[0]  # First row
first_three = df.iloc[:3]  # First three rows
print(f"\nFirst row:\n{first_row}")

# Row selection by label (if custom index)
df_with_index = df.set_index('Name')
alice_data = df_with_index.loc['Alice']
print(f"\nAlice's data:\n{alice_data}")

# Boolean indexing / filtering
# Simple condition
high_salary = df[df['Salary'] > 80000]
print(f"\nEmployees with salary > 80000:\n{high_salary}")

# Multiple conditions
engineering_high_salary = df[(df['Department'] == 'Engineering') & (df['Salary'] > 80000)]
print(f"\nEngineering employees with high salary:\n{engineering_high_salary}")

# Using isin() for multiple values
major_cities = df[df['City'].isin(['New York', 'London', 'Tokyo'])]
print(f"\nEmployees in major cities:\n{major_cities}")

# String operations
ny_employees = df[df['City'].str.contains('New')]
print(f"\nEmployees in cities containing 'New':\n{ny_employees}")

# Complex filtering with query method
complex_filter = df.query('Age > 28 and Salary < 90000')
print(f"\nComplex filter using query:\n{complex_filter}")

# Conditional selection with np.where
df['Seniority'] = np.where(df['Age'] > 30, 'Senior', 'Junior')
print(f"\nDataFrame with seniority:\n{df}")

# Advanced selection with loc and iloc
# Select specific rows and columns
specific_data = df.loc[df['Age'] > 28, ['Name', 'Department', 'Salary']]
print(f"\nSpecific data selection:\n{specific_data}")

# Using between for range filtering
age_range = df[df['Age'].between(25, 32, inclusive='both')]
print(f"\nEmployees aged 25-32:\n{age_range}")
```

## Example 3: Data Manipulation and Transformation
```python
# Adding and modifying columns
# Simple calculation
df['Annual_Bonus'] = df['Salary'] * 0.1
df['Total_Compensation'] = df['Salary'] + df['Annual_Bonus']

# Conditional column creation
df['Performance_Band'] = pd.cut(df['Salary'], 
                               bins=[0, 75000, 85000, float('inf')], 
                               labels=['Standard', 'Good', 'Excellent'])

# Apply function to column
df['Name_Length'] = df['Name'].apply(len)
df['City_Upper'] = df['City'].apply(lambda x: x.upper())

# Map values
dept_codes = {'Engineering': 'ENG', 'Marketing': 'MKT', 'HR': 'HR', 'Finance': 'FIN'}
df['Dept_Code'] = df['Department'].map(dept_codes)

# String operations
df['Email'] = df['Name'].str.lower().str.replace(' ', '.') + '@company.com'

# Date operations
df['Hire_Date'] = pd.date_range('2020-01-01', periods=len(df), freq='90D')
df['Years_Employed'] = (datetime.now() - df['Hire_Date']).dt.days / 365.25

print("DataFrame with new columns:")
print(df.head())

# Sorting
df_sorted = df.sort_values(['Department', 'Salary'], ascending=[True, False])
print(f"\nSorted by Department and Salary:\n{df_sorted}")

# Ranking
df['Salary_Rank'] = df['Salary'].rank(ascending=False)
df['Salary_Rank_Pct'] = df['Salary'].rank(pct=True)

print(f"\nDataFrame with rankings:\n{df[['Name', 'Salary', 'Salary_Rank', 'Salary_Rank_Pct']]}")

# Binning numerical data
df['Age_Group'] = pd.cut(df['Age'], bins=[20, 30, 35, 50], labels=['20-30', '30-35', '35+'])

# Duplicate handling
# Create some duplicates for demonstration
df_with_dupes = pd.concat([df, df.iloc[:2]], ignore_index=True)
print(f"\nDataFrame with duplicates: {len(df_with_dupes)} rows")

# Remove duplicates
df_no_dupes = df_with_dupes.drop_duplicates()
print(f"After removing duplicates: {len(df_no_dupes)} rows")

# Remove duplicates based on specific columns
df_no_name_dupes = df_with_dupes.drop_duplicates(subset=['Name'])
print(f"After removing name duplicates: {len(df_no_name_dupes)} rows")
```

## Example 4: Grouping and Aggregation
```python
# Basic grouping
# Group by single column
dept_groups = df.groupby('Department')
print("Average salary by department:")
print(dept_groups['Salary'].mean())

# Multiple aggregations
dept_stats = dept_groups.agg({
    'Salary': ['mean', 'median', 'std', 'min', 'max'],
    'Age': ['mean', 'count'],
    'Years_Employed': 'mean'
})
print(f"\nDepartment statistics:\n{dept_stats}")

# Custom aggregation functions
def salary_range(series):
    return series.max() - series.min()

custom_agg = dept_groups.agg({
    'Salary': [salary_range, 'mean'],
    'Age': lambda x: x.max() - x.min()
})
print(f"\nCustom aggregations:\n{custom_agg}")

# Group by multiple columns
# Create more data for better grouping examples
extended_data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'],
    'Department': ['Engineering', 'Engineering', 'Marketing', 'Marketing', 'HR', 'HR', 'Finance', 'Finance'],
    'Level': ['Senior', 'Junior', 'Senior', 'Junior', 'Senior', 'Junior', 'Senior', 'Junior'],
    'Salary': [95000, 75000, 85000, 65000, 80000, 60000, 90000, 70000],
    'Performance': ['Excellent', 'Good', 'Good', 'Average', 'Excellent', 'Average', 'Good', 'Good']
}
df_extended = pd.DataFrame(extended_data)

# Group by multiple columns
multi_group = df_extended.groupby(['Department', 'Level'])
print(f"\nMulti-level grouping:\n{multi_group['Salary'].mean()}")

# Pivot tables
pivot_salary = df_extended.pivot_table(
    values='Salary', 
    index='Department', 
    columns='Level', 
    aggfunc='mean',
    fill_value=0
)
print(f"\nPivot table - Salary by Department and Level:\n{pivot_salary}")

# Advanced pivot with multiple values and functions
pivot_advanced = df_extended.pivot_table(
    values=['Salary'], 
    index=['Department'], 
    columns=['Level'],
    aggfunc={'Salary': ['mean', 'count']},
    fill_value=0
)
print(f"\nAdvanced pivot table:\n{pivot_advanced}")

# Transform operations
# Add group statistics to original DataFrame
df_extended['Dept_Avg_Salary'] = df_extended.groupby('Department')['Salary'].transform('mean')
df_extended['Salary_vs_Dept_Avg'] = df_extended['Salary'] - df_extended['Dept_Avg_Salary']

# Percentile ranks within groups
df_extended['Dept_Salary_Rank'] = df_extended.groupby('Department')['Salary'].rank(pct=True)

print(f"\nDataFrame with group statistics:\n{df_extended}")

# Filter groups
# Keep only departments with more than 1 employee
dept_counts = df_extended.groupby('Department').size()
large_depts = dept_counts[dept_counts > 1].index
df_large_depts = df_extended[df_extended['Department'].isin(large_depts)]

# Apply function to groups
def standardize_salary(group):
    group['Standardized_Salary'] = (group['Salary'] - group['Salary'].mean()) / group['Salary'].std()
    return group

df_standardized = df_extended.groupby('Department').apply(standardize_salary)
print(f"\nStandardized salaries by department:\n{df_standardized[['Name', 'Department', 'Salary', 'Standardized_Salary']]}")
```

## Example 5: Merging and Joining DataFrames
```python
# Create sample DataFrames for merging
employees_df = pd.DataFrame({
    'emp_id': [1, 2, 3, 4, 5],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'department_id': [101, 102, 101, 103, 102]
})

departments_df = pd.DataFrame({
    'dept_id': [101, 102, 103, 104],
    'dept_name': ['Engineering', 'Marketing', 'HR', 'Finance'],
    'budget': [500000, 300000, 200000, 400000]
})

projects_df = pd.DataFrame({
    'project_id': [1, 2, 3, 4],
    'project_name': ['WebApp', 'Mobile', 'Analytics', 'Security'],
    'lead_emp_id': [1, 3, 2, 1],
    'department_id': [101, 101, 102, 101]
})

# Inner join (default)
inner_merge = pd.merge(employees_df, departments_df, 
                      left_on='department_id', right_on='dept_id')
print("Inner join - employees with departments:")
print(inner_merge)

# Left join
left_merge = pd.merge(employees_df, departments_df, 
                     left_on='department_id', right_on='dept_id', 
                     how='left')
print(f"\nLeft join - all employees:\n{left_merge}")

# Right join
right_merge = pd.merge(employees_df, departments_df, 
                      left_on='department_id', right_on='dept_id', 
                      how='right')
print(f"\nRight join - all departments:\n{right_merge}")

# Outer join
outer_merge = pd.merge(employees_df, departments_df, 
                      left_on='department_id', right_on='dept_id', 
                      how='outer')
print(f"\nOuter join - all records:\n{outer_merge}")

# Multiple joins
# First join employees with departments
emp_dept = pd.merge(employees_df, departments_df, 
                   left_on='department_id', right_on='dept_id')

# Then join with projects
full_data = pd.merge(emp_dept, projects_df, 
                    left_on='emp_id', right_on='lead_emp_id', 
                    how='left')
print(f"\nMultiple joins - employees, departments, and projects:\n{full_data}")

# Join on index
df1 = pd.DataFrame({'A': [1, 2, 3]}, index=['x', 'y', 'z'])
df2 = pd.DataFrame({'B': [4, 5, 6]}, index=['x', 'y', 'w'])

index_join = df1.join(df2, how='outer')
print(f"\nJoin on index:\n{index_join}")

# Concatenation
# Vertical concatenation (stacking)
df_2020 = pd.DataFrame({
    'month': ['Jan', 'Feb', 'Mar'],
    'sales': [100, 120, 110],
    'year': [2020, 2020, 2020]
})

df_2021 = pd.DataFrame({
    'month': ['Jan', 'Feb', 'Mar'],
    'sales': [105, 125, 115],
    'year': [2021, 2021, 2021]
})

vertical_concat = pd.concat([df_2020, df_2021], ignore_index=True)
print(f"\nVertical concatenation:\n{vertical_concat}")

# Horizontal concatenation
horizontal_concat = pd.concat([df_2020[['month', 'sales']], 
                              df_2021[['sales']].rename(columns={'sales': 'sales_2021'})], 
                             axis=1)
print(f"\nHorizontal concatenation:\n{horizontal_concat}")

# Merge with suffixes for overlapping columns
overlap_merge = pd.merge(df_2020, df_2021, on='month', suffixes=('_2020', '_2021'))
print(f"\nMerge with suffixes:\n{overlap_merge}")
```

## Example 6: Time Series Operations
```python
# Create time series data
dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
ts_data = pd.DataFrame({
    'date': dates,
    'sales': np.random.normal(1000, 200, len(dates)),
    'customers': np.random.poisson(50, len(dates)),
    'temperature': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365) + np.random.normal(0, 3, len(dates))
})

# Set date as index
ts_data.set_index('date', inplace=True)
print("Time series data:")
print(ts_data.head())

# Time-based selection
january_data = ts_data['2023-01']
q1_data = ts_data['2023-01':'2023-03']
print(f"\nJanuary data shape: {january_data.shape}")
print(f"Q1 data shape: {q1_data.shape}")

# Resampling
# Monthly aggregations
monthly_stats = ts_data.resample('M').agg({
    'sales': ['sum', 'mean', 'std'],
    'customers': 'sum',
    'temperature': 'mean'
})
print(f"\nMonthly statistics:\n{monthly_stats.head()}")

# Weekly rolling averages
ts_data['sales_7day_avg'] = ts_data['sales'].rolling(window=7).mean()
ts_data['sales_30day_avg'] = ts_data['sales'].rolling(window=30).mean()

# Exponential weighted moving average
ts_data['sales_ewm'] = ts_data['sales'].ewm(span=10).mean()

print(f"\nTime series with moving averages:\n{ts_data[['sales', 'sales_7day_avg', 'sales_30day_avg', 'sales_ewm']].head(10)}")

# Seasonal decomposition
from scipy import signal
# Simple trend analysis
ts_data['sales_trend'] = ts_data['sales'].rolling(window=30, center=True).mean()

# Date/time components
ts_data['year'] = ts_data.index.year
ts_data['month'] = ts_data.index.month
ts_data['dayofweek'] = ts_data.index.dayofweek
ts_data['quarter'] = ts_data.index.quarter

# Seasonal analysis
seasonal_sales = ts_data.groupby('month')['sales'].mean()
print(f"\nSeasonal sales by month:\n{seasonal_sales}")

# Lag features
ts_data['sales_lag1'] = ts_data['sales'].shift(1)
ts_data['sales_lag7'] = ts_data['sales'].shift(7)

# Percentage change
ts_data['sales_pct_change'] = ts_data['sales'].pct_change()
ts_data['sales_pct_change_7d'] = ts_data['sales'].pct_change(periods=7)

print(f"\nTime series with lag features:\n{ts_data[['sales', 'sales_lag1', 'sales_lag7', 'sales_pct_change']].head(10)}")

# Time zone handling
ts_utc = ts_data.copy()
ts_utc.index = ts_utc.index.tz_localize('UTC')
ts_ny = ts_utc.tz_convert('America/New_York')
print(f"\nTime zone conversion example:\n{ts_ny.head()}")
```

## Example 7: Advanced DataFrame Operations
```python
# Multi-index DataFrames
# Create hierarchical index
arrays = [
    ['Sales', 'Sales', 'Marketing', 'Marketing', 'Engineering', 'Engineering'],
    ['Q1', 'Q2', 'Q1', 'Q2', 'Q1', 'Q2']
]
tuples = list(zip(*arrays))
index = pd.MultiIndex.from_tuples(tuples, names=['Department', 'Quarter'])

multi_df = pd.DataFrame({
    'Revenue': [100000, 120000, 80000, 95000, 150000, 180000],
    'Costs': [60000, 70000, 50000, 55000, 90000, 100000],
    'Employees': [10, 12, 8, 9, 20, 22]
}, index=index)

print("Multi-index DataFrame:")
print(multi_df)

# Accessing multi-index data
sales_data = multi_df.loc['Sales']
q1_data = multi_df.xs('Q1', level='Quarter')
print(f"\nSales department data:\n{sales_data}")
print(f"\nQ1 data across departments:\n{q1_data}")

# Unstacking multi-index
unstacked = multi_df.unstack('Quarter')
print(f"\nUnstacked DataFrame:\n{unstacked}")

# Stack/unstack operations
stacked = unstacked.stack('Quarter')
print(f"\nRe-stacked DataFrame:\n{stacked}")

# Cross-tabulation
sample_data = pd.DataFrame({
    'Gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'] * 10,
    'Department': ['Sales', 'Marketing', 'Engineering', 'HR'] * 20,
    'Performance': np.random.choice(['Excellent', 'Good', 'Average'], 80)
})

crosstab = pd.crosstab(sample_data['Department'], sample_data['Performance'], 
                      margins=True, normalize='index')
print(f"\nCross-tabulation (normalized by row):\n{crosstab}")

# Memory optimization
# Check memory usage
print(f"\nMemory usage before optimization:")
print(sample_data.memory_usage(deep=True))

# Optimize data types
sample_data['Gender'] = sample_data['Gender'].astype('category')
sample_data['Department'] = sample_data['Department'].astype('category')
sample_data['Performance'] = sample_data['Performance'].astype('category')

print(f"\nMemory usage after optimization:")
print(sample_data.memory_usage(deep=True))

# Advanced filtering with eval and query
large_df = pd.DataFrame({
    'A': np.random.randn(10000),
    'B': np.random.randn(10000),
    'C': np.random.randn(10000),
    'D': np.random.randn(10000)
})

# Using eval for complex expressions
large_df['Complex_Calc'] = large_df.eval('A * B + C / D')

# Using query for efficient filtering
filtered = large_df.query('A > 0 and B < 0 and Complex_Calc > 0')
print(f"\nFiltered data shape: {filtered.shape}")

# Window functions
large_df['Row_Number'] = range(1, len(large_df) + 1)
large_df['A_Rolling_Mean'] = large_df['A'].rolling(window=100).mean()
large_df['A_Rank'] = large_df['A'].rank()
large_df['A_Pct_Rank'] = large_df['A'].rank(pct=True)

# Cumulative operations
large_df['A_Cumsum'] = large_df['A'].cumsum()
large_df['A_Cummax'] = large_df['A'].cummax()
large_df['A_Cummin'] = large_df['A'].cummin()

print(f"\nDataFrame with window functions:\n{large_df[['A', 'A_Rolling_Mean', 'A_Rank', 'A_Pct_Rank']].head()}")

# Performance considerations
# Using vectorized operations vs apply
def slow_function(x):
    return x ** 2 + np.sin(x)

# Vectorized (fast)
%timeit large_df['Vectorized'] = large_df['A'] ** 2 + np.sin(large_df['A'])

# Apply (slower)
%timeit large_df['Applied'] = large_df['A'].apply(slow_function)

# Chunking for large datasets
def process_chunk(chunk):
    chunk['Processed'] = chunk['A'] * 2
    return chunk

chunk_size = 1000
processed_chunks = []
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    processed_chunk = process_chunk(chunk)
    processed_chunks.append(processed_chunk)

# Combine chunks
final_df = pd.concat(processed_chunks, ignore_index=True)
```

## Usage Notes:
- **Example 1**: DataFrame creation from various sources, basic operations, and missing data handling
- **Example 2**: Data selection techniques including boolean indexing, loc/iloc, and conditional selection
- **Example 3**: Data manipulation including adding columns, transformations, sorting, and duplicate handling
- **Example 4**: Grouping operations, aggregations, pivot tables, and transform operations
- **Example 5**: Merging and joining DataFrames with various join types and concatenation
- **Example 6**: Time series operations including resampling, rolling windows, and seasonal analysis
- **Example 7**: Advanced operations including multi-index, cross-tabulation, memory optimization, and performance considerations

These examples demonstrate the full spectrum of pandas DataFrame capabilities from basic data manipulation to advanced analytics and performance optimization techniques.