# PANDAS DATAFRAMES COMPREHENSIVE GUIDE

## 1. INTRODUCTION TO PANDAS DATAFRAMES

Pandas is a powerful Python library for data manipulation and analysis. At its core is the DataFrame, a two-dimensional tabular data structure with labeled axes (rows and columns), similar to a spreadsheet or SQL table.

### Installation and Setup
```python
# Install pandas
pip install pandas

# Basic imports
import pandas as pd
import numpy as np
```

## 2. CREATING DATAFRAMES

### From Python Dictionaries
```python
# From dictionary of lists/arrays
data = {
    'name': ['Alice', 'Bob', 'Charlie', 'David'],
    'age': [25, 30, 35, 40],
    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']
}
df = pd.DataFrame(data)

# With custom index
df = pd.DataFrame(data, index=['person1', 'person2', 'person3', 'person4'])

# From dict of dicts
nested_dict = {
    'A': {'x': 1, 'y': 2},
    'B': {'x': 3, 'y': 4, 'z': 5}
}
df = pd.DataFrame(nested_dict)
```

### From Lists
```python
# From list of lists
data = [
    ['Alice', 25, 'New York'],
    ['Bob', 30, 'Los Angeles'],
    ['Charlie', 35, 'Chicago']
]
df = pd.DataFrame(data, columns=['name', 'age', 'city'])

# From list of dicts
data = [
    {'name': 'Alice', 'age': 25, 'city': 'New York'},
    {'name': 'Bob', 'age': 30, 'city': 'Los Angeles'},
    {'name': 'Charlie', 'age': 35} # Note: missing 'city' will show as NaN
]
df = pd.DataFrame(data)
```

### From NumPy Arrays
```python
# From NumPy array
array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
df = pd.DataFrame(array, columns=['A', 'B', 'C'])

# From random data
df = pd.DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])
```

### From Files
```python
# CSV file
df = pd.read_csv('data.csv')

# Excel file (requires openpyxl or xlrd)
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# JSON file
df = pd.read_json('data.json')

# SQL query (requires SQLAlchemy)
from sqlalchemy import create_engine
engine = create_engine('sqlite:///database.db')
df = pd.read_sql("SELECT * FROM table_name", engine)
df = pd.read_sql_table("table_name", engine)

# HTML table
df = pd.read_html('https://example.com/table.html')[0]  # Returns a list of all tables
```

### Empty DataFrames
```python
# Empty DataFrame with column names
df = pd.DataFrame(columns=['name', 'age', 'city'])

# DataFrame with specific data types
df = pd.DataFrame({
    'int_col': pd.Series(dtype='int'),
    'float_col': pd.Series(dtype='float'),
    'str_col': pd.Series(dtype='str'),
    'date_col': pd.Series(dtype='datetime64[ns]')
})
```

## 3. BASIC INFORMATION AND ATTRIBUTES

```python
# Size and shape
df.shape  # (rows, columns)
len(df)   # Number of rows
df.size   # Total number of elements (rows * columns)

# Column and index information
df.columns  # Column labels
df.index    # Row labels
df.axes     # Both row and column labels

# Data types
df.dtypes   # Data types of each column
df.info()   # Summary including dtypes and non-null values

# Summary statistics
df.describe()  # Statistical summary of numerical columns
df.describe(include='all')  # Include non-numeric columns

# Memory usage
df.memory_usage()  # Memory usage of each column
df.info(memory_usage='deep')  # Detailed memory usage
```

## 4. SELECTING AND ACCESSING DATA

### Column Selection
```python
# Single column - returns Series
s = df['name']
s = df.name  # Dot notation (only works for valid Python identifiers)

# Multiple columns - returns DataFrame
subset = df[['name', 'age']]
```

### Row Selection by Position
```python
# Single row by integer position
row = df.iloc[0]  # First row

# Multiple rows by positions
subset = df.iloc[0:3]  # First three rows
subset = df.iloc[[0, 2, 4]]  # Specific rows (1st, 3rd, 5th)
```

### Row Selection by Label
```python
# When using custom index
df = pd.DataFrame(data, index=['a', 'b', 'c', 'd'])

# Single row by label
row = df.loc['a']

# Multiple rows by labels
subset = df.loc['a':'c']  # Inclusive of 'c'!
subset = df.loc[['a', 'c']]
```

### Combined Row and Column Selection
```python
# iloc: Select by integer position
value = df.iloc[0, 1]  # First row, second column
subset = df.iloc[0:2, 1:3]  # First two rows, second and third columns

# loc: Select by label
value = df.loc['a', 'age']  # Row 'a', column 'age'
subset = df.loc['a':'c', ['name', 'city']]  # Rows a-c, columns name and city
```

### Boolean Indexing
```python
# Filter rows based on a condition
adults = df[df['age'] >= 18]
new_yorkers = df[df['city'] == 'New York']

# Multiple conditions
subset = df[(df['age'] > 30) & (df['city'] == 'Chicago')]
subset = df[(df['age'] < 25) | (df['city'] == 'Houston')]

# Using query method (more readable for complex conditions)
subset = df.query("age > 30 and city == 'Chicago'")
subset = df.query("age < 25 or city == 'Houston'")

# Check if values are in a list
cities = ['New York', 'Chicago']
subset = df[df['city'].isin(cities)]
subset = df[~df['city'].isin(cities)]  # Negation with ~
```

### At and iat (For Single Value Access)
```python
# At - label based access (faster than loc for single value)
value = df.at['a', 'age']

# iat - integer position based access (faster than iloc for single value)
value = df.iat[0, 1]
```

## 5. DATA MANIPULATION

### Adding and Removing Columns
```python
# Add a new column
df['salary'] = [50000, 60000, 70000, 80000]

# Add a calculated column
df['age_in_months'] = df['age'] * 12
df['full_name'] = df['first_name'] + ' ' + df['last_name']

# Delete columns
df_copy = df.drop('salary', axis=1)  # axis=1 specifies columns
df_copy = df.drop(['age', 'city'], axis=1)  # Multiple columns

# Delete in-place
df.drop('salary', axis=1, inplace=True)
```

### Adding and Removing Rows
```python
# Add a row using loc
df.loc['new_row'] = ['Eve', 45, 'Miami']

# Add a row using DataFrame.append (deprecated in newer versions)
new_row = pd.DataFrame([['Eve', 45, 'Miami']], columns=df.columns, index=['new_row'])
df = pd.concat([df, new_row])  # Preferred approach

# Delete rows
df_copy = df.drop('new_row')  # By index label
df_copy = df.drop([0, 2])  # By index position (if using default integer index)
```

### Renaming Columns and Indices
```python
# Rename columns
df = df.rename(columns={'name': 'full_name', 'city': 'location'})
df.columns = ['full_name', 'age', 'location']  # Replace all column names

# Rename index
df = df.rename(index={'a': 'person1', 'b': 'person2'})

# Reset index (convert index to column and create new sequential index)
df = df.reset_index()
df = df.reset_index(drop=True)  # Don't keep old index as column
```

### Reordering Columns
```python
# Reorder columns
df = df[['city', 'name', 'age']]  # Explicit order

# Sort columns alphabetically
df = df.sort_index(axis=1)
```

## 6. DATA CLEANING

### Handling Missing Values
```python
# Detect missing values
df.isna()  # Returns DataFrame of booleans
df.isnull()  # Alias for isna()
df.isna().sum()  # Count of missing values in each column
df.isna().any()  # Check if each column has any missing values

# Drop rows with missing values
df_clean = df.dropna()  # Drop rows with ANY missing values
df_clean = df.dropna(subset=['age', 'city'])  # Only check specific columns
df_clean = df.dropna(thresh=2)  # Drop rows with less than 2 non-NA values

# Drop columns with missing values
df_clean = df.dropna(axis=1)  # Drop columns with ANY missing values

# Fill missing values
df_filled = df.fillna(0)  # Fill all missing with 0
df_filled = df.fillna({'age': 0, 'city': 'Unknown'})  # Different values by column
df_filled = df.copy()
df_filled['age'] = df_filled['age'].fillna(df['age'].mean())  # Fill with mean

# Forward/backward fill
df_filled = df.fillna(method='ffill')  # Forward fill (use previous value)
df_filled = df.fillna(method='bfill')  # Backward fill (use next value)

# Interpolation
df_filled = df.interpolate()  # Linear interpolation for numeric columns
df_filled = df.interpolate(method='cubic')  # Cubic interpolation
```

### Removing Duplicates
```python
# Check for duplicates
df.duplicated()  # Returns boolean Series
df.duplicated().sum()  # Count of duplicate rows

# Find duplicate rows
duplicates = df[df.duplicated()]
duplicates = df[df.duplicated(keep=False)]  # All duplicates (not just second occurrence)

# Remove duplicates
df_unique = df.drop_duplicates()
df_unique = df.drop_duplicates(subset=['name', 'city'])  # Check only specific columns
df_unique = df.drop_duplicates(keep='last')  # Keep last occurrence
```

### Data Type Conversion
```python
# Convert column types
df['age'] = df['age'].astype('float')
df['is_adult'] = df['is_adult'].astype('bool')
df['id'] = df['id'].astype('str')

# Convert to datetime
df['date'] = pd.to_datetime(df['date'])
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Invalid dates become NaT

# Multiple columns at once
df = df.astype({'age': 'int', 'salary': 'float', 'name': 'string'})

# Categorical data (memory efficient)
df['city'] = df['city'].astype('category')
```

### String Operations
```python
# Using str accessor for string operations
df['name'] = df['name'].str.upper()
df['name'] = df['name'].str.lower()
df['name'] = df['name'].str.title()
df['name'] = df['name'].str.strip()
df['initial'] = df['name'].str[0]  # First character
df['name_length'] = df['name'].str.len()

# Replace substrings
df['text'] = df['text'].str.replace('old', 'new')
df['text'] = df['text'].str.replace('pattern', 'replacement', regex=True)

# Extract patterns with regex
df['zip_code'] = df['address'].str.extract(r'(\d{5})')  # Extract 5-digit codes
```

## 7. SORTING AND ORDERING

```python
# Sort by columns
df_sorted = df.sort_values('age')  # Ascending by default
df_sorted = df.sort_values('age', ascending=False)  # Descending
df_sorted = df.sort_values(['city', 'age'])  # Multiple columns
df_sorted = df.sort_values(['city', 'age'], ascending=[True, False])  # Different directions

# Sort by index
df_sorted = df.sort_index()
df_sorted = df.sort_index(ascending=False)

# Get smallest/largest values
top3 = df.nsmallest(3, 'age')
bottom3 = df.nlargest(3, 'salary')
```

## 8. GROUPING AND AGGREGATION

### GroupBy Operations
```python
# Group by single column
grouped = df.groupby('city')

# Group by multiple columns
grouped = df.groupby(['city', 'gender'])

# Iterate through groups
for name, group in grouped:
    print(name)
    print(group)

# Basic statistics for each group
grouped.mean()  # Mean of numeric columns by group
grouped.size()  # Count of rows in each group
grouped.count()  # Count of non-NA values in each column by group

# Apply multiple aggregations
result = grouped.agg(['count', 'mean', 'min', 'max'])

# Different aggregations per column
result = grouped.agg({
    'age': ['min', 'max', 'mean'],
    'salary': ['mean', 'median', 'std']
})

# Named aggregations (pandas >= 0.25)
result = grouped.agg(
    min_age=('age', 'min'),
    max_age=('age', 'max'),
    avg_salary=('salary', 'mean'),
    count=('name', 'count')
)

# Custom aggregation functions
result = grouped.agg(lambda x: x.max() - x.min())
```

### Filter, Transform and Apply
```python
# Filter: Keep groups that satisfy a condition
result = grouped.filter(lambda x: x['age'].mean() > 30)

# Transform: Apply function to each group and return same-sized DataFrame
result = grouped.transform(lambda x: (x - x.mean()) / x.std())  # Z-score normalization

# Apply: Apply arbitrary function to each group
result = grouped.apply(lambda x: x.iloc[0])  # First row of each group
```

### Pivot Tables
```python
# Simple pivot table
pivot = df.pivot(index='city', columns='gender', values='salary')

# Pivot table with aggregation
pivot = pd.pivot_table(
    df, 
    index='city', 
    columns='gender',
    values='salary', 
    aggfunc='mean'
)

# Multiple values and aggregations
pivot = pd.pivot_table(
    df, 
    index=['city', 'department'], 
    columns='gender',
    values=['salary', 'age'], 
    aggfunc={'salary': 'mean', 'age': ['min', 'max']}
)
```

## 9. JOINING AND MERGING DATAFRAMES

### Concatenation
```python
# Concatenate rows (vertically)
df_combined = pd.concat([df1, df2])
df_combined = pd.concat([df1, df2], ignore_index=True)  # Reset index

# Concatenate columns (horizontally)
df_combined = pd.concat([df1, df2], axis=1)

# Join behavior
df_combined = pd.concat([df1, df2], join='inner')  # Only common columns
df_combined = pd.concat([df1, df2], join='outer')  # All columns (default)
```

### Merge (SQL-like Joins)
```python
# Inner join (only matching keys)
merged = pd.merge(df1, df2, on='id')
merged = pd.merge(df1, df2, on=['id', 'name'])  # Multiple keys

# Outer join (all keys)
merged = pd.merge(df1, df2, on='id', how='outer')

# Left join (all rows from left, matching from right)
merged = pd.merge(df1, df2, on='id', how='left')

# Right join (all rows from right, matching from left)
merged = pd.merge(df1, df2, on='id', how='right')

# Join on differently named columns
merged = pd.merge(df1, df2, left_on='id_a', right_on='id_b')

# Join on index
merged = pd.merge(df1, df2, left_index=True, right_index=True)
merged = pd.merge(df1, df2, left_index=True, right_on='id')

# Indicator to show merge source
merged = pd.merge(df1, df2, on='id', how='outer', indicator=True)

# Suffixes for overlapping columns
merged = pd.merge(df1, df2, on='id', suffixes=('_left', '_right'))
```

### Join Method
```python
# Using DataFrame.join() - joins on indices by default
joined = df1.join(df2)  # Left join by default
joined = df1.join(df2, how='inner')

# Join with a specified key
joined = df1.join(df2, on='key', lsuffix='_x', rsuffix='_y')
```

## 10. RESHAPING DATA

### Stack and Unstack
```python
# Stack: Pivot columns to rows (wide to long)
stacked = df.stack()  # Multi-index Series with row index and column names

# Unstack: Pivot rows to columns (long to wide)
unstacked = stacked.unstack()  # Back to original
unstacked = df.unstack()  # Second level of index becomes columns
```

### Melt
```python
# Melt: Convert columns to rows (wide to long)
melted = pd.melt(df, 
                id_vars=['name', 'city'],  # Columns to keep as is
                value_vars=['2020', '2021', '2022'],  # Columns to "melt"
                var_name='year',  # Name for variable column
                value_name='revenue')  # Name for value column

# Melt all columns except id_vars
melted = pd.melt(df, id_vars=['name', 'city'])
```

### Pivot
```python
# Pivot: Reshape long to wide format
# Start with melted data
pivot = melted.pivot(index='name', columns='year', values='revenue')
```

### Explode
```python
# Explode a column with lists into multiple rows
df = pd.DataFrame({
    'name': ['Alice', 'Bob'],
    'skills': [['Python', 'SQL'], ['Java', 'C++', 'SQL']]
})
exploded = df.explode('skills')
```

## 11. TIME SERIES FUNCTIONALITY

### Working with Datetime Index
```python
# Create DatetimeIndex
dates = pd.date_range(start='2023-01-01', periods=10, freq='D')
df = pd.DataFrame({'value': range(10)}, index=dates)

# Date properties
df.index.year
df.index.month
df.index.day
df.index.day_name()
df.index.dayofweek  # Monday=0, Sunday=6
df.index.quarter

# Filter by date
df['2023-01-05':'2023-01-10']  # Slice by date
df[df.index.month == 1]  # All January dates
df[df.index.dayofweek < 5]  # Weekdays only
```

### Resampling
```python
# Downsample: reduce frequency
monthly = df.resample('M').mean()  # Monthly average
weekly = df.resample('W').sum()    # Weekly sum

# Upsample: increase frequency
daily = monthly.resample('D').ffill()  # Fill forward
hourly = daily.resample('H').interpolate()  # Interpolate values
```

### Shifting and Lagging
```python
# Shift values
df_shifted = df.shift(1)  # Shift down by 1 (previous day's values)
df_shifted = df.shift(-1)  # Shift up by 1 (next day's values)

# Calculate difference with previous row
df['change'] = df['value'] - df['value'].shift(1)
df['pct_change'] = df['value'].pct_change()  # Percentage change

# Rolling windows
df['3day_avg'] = df['value'].rolling(window=3).mean()
df['7day_sum'] = df['value'].rolling(window=7).sum()
df['30day_std'] = df['value'].rolling(window=30).std()
```

### Date Offsets and Business Days
```python
# Use date offsets
from pandas.tseries.offsets import Day, BusinessDay, MonthEnd

df.shift(1, freq=Day())
df.shift(1, freq=BusinessDay())  # Skip weekends
df.shift(1, freq=MonthEnd())  # Next month end

# Business day functionality
from pandas.tseries.holiday import USFederalHolidayCalendar
cal = USFederalHolidayCalendar()
holidays = cal.holidays(start='2023-01-01', end='2023-12-31')
df_business = df[~df.index.isin(holidays)]  # Exclude holidays
```

## 12. ADVANCED OPERATIONS

### Apply and Map Functions
```python
# Apply function to each element in a Series
df['age_squared'] = df['age'].apply(lambda x: x**2)

# Apply function to each row of DataFrame
df['name_age'] = df.apply(lambda row: f"{row['name']} is {row['age']} years old", axis=1)

# Map values using dictionary
mapping = {'New York': 'East', 'Chicago': 'Midwest', 'Los Angeles': 'West'}
df['region'] = df['city'].map(mapping)

# Replace values
df['status'] = df['status'].replace({'active': 1, 'inactive': 0})
```

### Window Functions
```python
# Rolling statistics
df['rolling_mean'] = df['value'].rolling(window=3).mean()
df['rolling_std'] = df['value'].rolling(window=5).std()

# Expanding window
df['cumulative_mean'] = df['value'].expanding().mean()
df['cumulative_sum'] = df['value'].expanding().sum()

# Exponential weighted functions
df['ewm'] = df['value'].ewm(span=4).mean()
```

### Cross-tabulation
```python
# Simple crosstab
pd.crosstab(df['gender'], df['city'])

# With aggregation
pd.crosstab(df['gender'], df['city'], values=df['salary'], aggfunc='mean')

# Normalize
pd.crosstab(df['gender'], df['city'], normalize='index')  # Row percentages
pd.crosstab(df['gender'], df['city'], normalize='columns')  # Column percentages
```

### Categorical Data
```python
# Create categorical data
df['color'] = pd.Categorical(['red', 'green', 'blue', 'red'])

# With ordered categories
df['size'] = pd.Categorical(
    ['medium', 'large', 'small', 'large'],
    categories=['small', 'medium', 'large'],
    ordered=True
)

# Compare ordered categories
mask = df['size'] > 'small'
```

## 13. PERFORMANCE OPTIMIZATION

### Memory Reduction
```python
# Check memory usage
df.info(memory_usage='deep')

# Use appropriate dtypes
df_optimized = df.copy()
df_optimized['id'] = df_optimized['id'].astype('int32')  # Smaller integer type
df_optimized['name'] = df_optimized['name'].astype('category')  # Category for repeated strings

# Convert float to smaller precision
df_optimized['amount'] = df_optimized['amount'].astype('float32')
```

### Computation Optimization
```python
# Vectorization instead of loops
# Slower:
for i in range(len(df)):
    df.loc[i, 'new_col'] = df.loc[i, 'col1'] * 2

# Faster:
df['new_col'] = df['col1'] * 2

# Use numba for intensive calculations
import numba

@numba.jit(nopython=True)
def fast_calculation(array):
    result = np.empty_like(array)
    for i in range(len(array)):
        result[i] = array[i] ** 2 + array[i] - 5
    return result

df['complex_result'] = fast_calculation(df['value'].values)
```

### Chunking Large Files
```python
# Process large CSV in chunks
chunks = []
for chunk in pd.read_csv('large_file.csv', chunksize=10000):
    # Process each chunk
    processed = chunk[chunk['value'] > 0]
    chunks.append(processed)

# Combine processed chunks
result = pd.concat(chunks)
```

## 14. DATA VISUALIZATION WITH PANDAS

### Basic Plots
```python
# Line plot
df.plot(y='value')

# Multiple columns
df.plot(y=['value1', 'value2'])

# Bar plot
df.plot(kind='bar')
df.plot.bar()  # Alternative syntax

# Horizontal bar
df.plot(kind='barh')
df.plot.barh()

# Histogram
df['value'].plot(kind='hist', bins=20)
df.plot.hist(alpha=0.5)  # Overlay multiple histograms

# Scatter plot
df.plot(kind='scatter', x='value1', y='value2')
df.plot.scatter(x='value1', y='value2')
```

### Advanced Plots
```python
# Box plot
df.plot(kind='box')

# Area plot
df.plot(kind='area', alpha=0.5)

# Pie chart
df.plot(kind='pie', y='value', figsize=(10, 10))

# Hexbin plot (for density)
df.plot.hexbin(x='value1', y='value2', gridsize=20)
```

### Customization
```python
# Figure size and title
df.plot(figsize=(10, 6), title='My Plot')

# Labels and legend
df.plot(
    x='date', 
    y='value',
    xlabel='Date', 
    ylabel='Value',
    legend=True
)

# Style and color
df.plot(style='.-', color=['red', 'blue', 'green'])

# Save plot
plot = df.plot()
fig = plot.get_figure()
fig.savefig('my_plot.png', dpi=300)
```

## 15. IMPORTING AND EXPORTING DATA

### Reading Data
```python
# Read with options
df = pd.read_csv(
    'data.csv',
    sep=',',              # Delimiter
    header=0,             # Row number to use as header
    index_col=0,          # Column to use as index
    skiprows=[1, 2],      # Skip specific rows
    nrows=100,            # Number of rows to read
    na_values=['NA', '?'],  # Custom NA values
    dtype={'id': int, 'name': str}  # Column dtypes
)

# Read Excel with specific sheets and ranges
df = pd.read_excel(
    'data.xlsx',
    sheet_name='Sheet1',
    skiprows=2,
    usecols='A:C',
    nrows=10
)

# Read JSON with orientation
df = pd.read_json('data.json', orient='records')
```

### Writing Data
```python
# Write to CSV
df.to_csv(
    'output.csv',
    index=False,     # Don't write index
    header=True,     # Write header row
    sep=',',         # Delimiter
    na_rep='NULL',   # Representation for NA
    float_format='%.2f'  # Format for floats
)

# Write to Excel
df.to_excel(
    'output.xlsx',
    sheet_name='Sheet1',
    index=False,
    freeze_panes=(1, 0)  # Freeze first row
)

# Write to JSON
df.to_json(
    'output.json',
    orient='records',  # [{column -> value}, ... , {column -> value}]
    indent=4          # Pretty print
)

# Write to SQL
df.to_sql(
    'table_name',
    engine,
    if_exists='replace',  # 'fail', 'replace', or 'append'
    index=False,
    chunksize=1000       # Write in chunks
)
```

## 16. PRACTICAL EXAMPLES

### Data Analysis Pipeline
```python
# Complete analysis pipeline
def analyze_sales_data(filepath):
    # 1. Load data
    df = pd.read_csv(filepath)
    
    # 2. Clean data
    df = df.dropna()
    df['date'] = pd.to_datetime(df['date'])
    df['amount'] = df['amount'].astype(float)
    
    # 3. Feature engineering
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['is_weekend'] = df['date'].dt.dayofweek >= 5
    
    # 4. Aggregation
    monthly_sales = df.groupby(['year', 'month']).agg({
        'amount': ['sum', 'mean', 'count'],
        'customer_id': 'nunique'
    })
    
    # 5. Analysis
    top_products = df.groupby('product_id')['amount'].sum().nlargest(5)
    sales_growth = monthly_sales['amount']['sum'].pct_change()
    
    return {
        'monthly_sales': monthly_sales,
        'top_products': top_products,
        'sales_growth': sales_growth
    }
```

### Working with Real Data
```python
# Sales forecasting example
def forecast_sales(df):
    # 1. Prepare time series data
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    daily_sales = df.resample('D')['sales'].sum()
    
    # 2. Handle missing values
    daily_sales = daily_sales.fillna(daily_sales.rolling(7, min_periods=1).mean())
    
    # 3. Extract features
    daily_sales = pd.DataFrame(daily_sales)
    daily_sales['day_of_week'] = daily_sales.index.dayofweek
    daily_sales['month'] = daily_sales.index.month
    daily_sales['year'] = daily_sales.index.year
    
    # 4. Add lag features
    for lag in [1, 7, 14, 28]:
        daily_sales[f'lag_{lag}'] = daily_sales['sales'].shift(lag)
    
    # 5. Moving averages
    for window in [7, 14, 28]:
        daily_sales[f'ma_{window}'] = daily_sales['sales'].rolling(window).mean()
    
    # 6. Remove rows with NaN due to lag features
    daily_sales = daily_sales.dropna()
    
    # 7. Split into features and target
    features = daily_sales.drop('sales', axis=1)
    target = daily_sales['sales']
    
    return features, target
```

### Custom Data Transformation
```python
# Creating a custom transformation function
def transform_data(df):
    # Make a copy to avoid modifying the original
    result = df.copy()
    
    # 1. Drop unnecessary columns
    result.drop(['id', 'created_at'], axis=1, inplace=True)
    
    # 2. Handle missing values differently for each column
    result['numeric_col'].fillna(result['numeric_col'].median(), inplace=True)
    result['categorical_col'].fillna('Unknown', inplace=True)
    
    # 3. Create new features
    result['ratio'] = result['value_a'] / result['value_b']
    result['log_value'] = np.log1p(result['value'])
    
    # 4. Encode categorical variables
    for col in ['category1', 'category2']:
        dummies = pd.get_dummies(result[col], prefix=col, drop_first=True)
        result = pd.concat([result, dummies], axis=1)
        result.drop(col, axis=1, inplace=True)
    
    # 5. Normalize numeric features
    numeric_cols = ['value_a', 'value_b', 'ratio']
    for col in numeric_cols:
        result[col] = (result[col] - result[col].mean()) / result[col].std()
    
    return result
```

## 17. BEST PRACTICES

### Code Efficiency
```python
# Use vectorized operations
# Good:
df['total'] = df['price'] * df['quantity']

# Avoid:
for i in range(len(df)):
    df.loc[i, 'total'] = df.loc[i, 'price'] * df.loc[i, 'quantity']

# Use query for complex filtering
# Good:
result = df.query("age > 25 and city == 'New York'")

# Instead of:
result = df[(df['age'] > 25) & (df['city'] == 'New York')]
```

### Memory Management
```python
# Use appropriate data types
df['id'] = df['id'].astype('int32')  # If values fit in 32 bits
df['category'] = df['category'].astype('category')  # For strings with repeated values

# Process large files in chunks
reader = pd.read_csv('large_file.csv', chunksize=10000)
for chunk in reader:
    # Process each chunk
    process_data(chunk)

# Clean up after large operations
import gc
del large_df  # Delete reference to large DataFrame
gc.collect()  # Run garbage collector
```

### Performance Tips
```python
# For selecting rows, .loc is faster than boolean indexing
# Good:
subset = df.loc[df['value'] > 10]

# Use inplace=True for operations on large DataFrames
df.fillna(0, inplace=True)  # Modifies df directly, no copy

# Use efficient merging strategies
# If joining on index, use join() instead of merge()
joined = df1.join(df2)  # Instead of pd.merge(df1, df2, left_index=True, right_index=True)

# Avoid chained indexing
# Bad:
df['column'][df['column'] > 0] = 5  # Can cause SettingWithCopyWarning

# Good:
df.loc[df['column'] > 0, 'column'] = 5
```

## 18. TROUBLESHOOTING COMMON ISSUES

### Handling Warnings
```python
# SettingWithCopyWarning
# Problem:
temp = df[df['value'] > 0]  # Creates a view
temp['new_col'] = 1  # Ambiguous: modifies view or original?

# Solution:
temp = df[df['value'] > 0].copy()  # Create explicit copy
temp['new_col'] = 1  # Now clearly modifies temp

# Or use .loc for direct assignment
df.loc[df['value'] > 0, 'new_col'] = 1  # Modifies original directly
```

### Debugging Tips
```python
# Check data types
print(df.dtypes)

# Inspect problematic values
print(df[df['column'].isnull()])

# Check for NaN or infinite values
print(df.isna().sum())
print(np.isinf(df['column']).sum())

# Examine unique values
print(df['column'].unique())
print(df['column'].value_counts())

# Verify shapes after operations
print(f"Before: {df1.shape}, After: {result.shape}")
```

### Common Pitfalls
```python
# Forgetting about copy vs view
view = df[['col1', 'col2']]  # Creates a view, changes may affect original
copy = df[['col1', 'col2']].copy()  # Creates a copy, changes won't affect original

# Integer indexing in loc
# This won't work if index is string labels:
df.loc[0:3]  # Uses LABEL-based slicing, inclusive of end

# Use iloc for integer position indexing:
df.iloc[0:3]  # Uses POSITION-based slicing, exclusive of end
```

## 19. EXTENSIONS AND ECOSYSTEM

### Pandas Ecosystem
```python
# pandas-profiling for automatic EDA
import pandas_profiling
profile = pandas_profiling.ProfileReport(df)
profile.to_file("report.html")

# pandasql for SQL queries on DataFrames
from pandasql import sqldf
q = """SELECT * FROM df WHERE age > 30 ORDER BY age DESC"""
result = sqldf(q, globals())

# modin for parallel processing
import modin.pandas as pd  # Drop-in replacement for pandas
df = pd.read_csv('large_file.csv')  # Processed in parallel
```

### Integration with Other Libraries
```python
# With Scikit-learn
from sklearn.preprocessing import StandardScaler
X = df[['feature1', 'feature2', 'feature3']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# With Matplotlib and Seaborn
import matplotlib.pyplot as plt
import seaborn as sns

fig, ax = plt.subplots(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', ax=ax)
plt.show()

# With PyArrow for performance
df = pd.read_parquet('data.parquet', engine='pyarrow')
df.to_parquet('output.parquet', engine='pyarrow', compression='snappy')
```

## 20. CONCLUSION

Pandas DataFrames are incredibly versatile and powerful for data manipulation and analysis. The key to effective DataFrame usage lies in understanding their core concepts:

1. **Selection and Indexing**: Master `.loc`, `.iloc`, and boolean indexing for efficient data access
2. **Transformation**: Learn to reshape, clean, and transform data using pandas' vectorized operations
3. **Aggregation**: Use groupby, pivot tables, and other tools for summarizing data
4. **Performance**: Optimize memory usage and computation with appropriate techniques

Remember that pandas is built on NumPy, so for very large datasets or highly specialized numerical operations, you might want to convert to NumPy arrays or consider specialized libraries like Dask for truly big data.

For the most up-to-date information, always refer to the [official pandas documentation](https://pandas.pydata.org/docs/).
