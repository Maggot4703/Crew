# Pandas - Usage Guide

## Getting Started

### Installation
```bash
# Install pandas
pip install pandas

# Install with additional dependencies
pip install pandas[all]

# Install specific extras
pip install pandas[performance,computation,fss,excel,parquet,feather,hdf5,spss,postgresql,mysql,sql-other,html,xml,plot,output_formatting,clipboard,compression]
```

### Importing and Basic Setup
```python
import pandas as pd
import numpy as np

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 20)

# Check version
print(pd.__version__)
```

### Creating Basic Data Structures
```python
# Create Series
s = pd.Series([1, 2, 3, 4, 5])
s_named = pd.Series([1, 2, 3], index=['a', 'b', 'c'], name='my_series')

# Create DataFrame
df = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': ['a', 'b', 'c', 'd'],
    'C': [1.1, 2.2, 3.3, 4.4]
})

# From dictionary
data = {'col1': [1, 2], 'col2': [3, 4]}
df = pd.DataFrame(data)

# From lists
df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])
```

## Common Operations

### Reading and Writing Data
```python
# Read CSV
df = pd.read_csv('file.csv')
df = pd.read_csv('file.csv', index_col=0, parse_dates=['date_column'])

# Read Excel
df = pd.read_excel('file.xlsx', sheet_name='Sheet1')

# Read JSON
df = pd.read_json('file.json')

# Read from SQL
import sqlite3
conn = sqlite3.connect('database.db')
df = pd.read_sql('SELECT * FROM table', conn)

# Write to CSV
df.to_csv('output.csv', index=False)

# Write to Excel
df.to_excel('output.xlsx', sheet_name='Sheet1', index=False)

# Write to JSON
df.to_json('output.json', orient='records')
```

### Data Inspection
```python
# Basic info
df.head()          # First 5 rows
df.tail(10)        # Last 10 rows
df.info()          # Data types and non-null counts
df.describe()      # Statistical summary
df.shape           # Dimensions (rows, columns)
df.columns         # Column names
df.index           # Row index
df.dtypes          # Data types of each column

# Check for missing values
df.isnull().sum()
df.isna().any()

# Unique values
df['column'].unique()
df['column'].nunique()
df['column'].value_counts()
```

### Data Selection and Indexing
```python
# Select columns
df['column_name']           # Single column (Series)
df[['col1', 'col2']]       # Multiple columns (DataFrame)

# Select rows by index
df.loc[0]                  # By label
df.iloc[0]                 # By position
df.loc[0:2]                # Slice by label
df.iloc[0:3]               # Slice by position

# Boolean indexing
df[df['column'] > 5]
df[df['column'].isin(['a', 'b'])]
df[(df['col1'] > 5) & (df['col2'] < 10)]

# Select specific cells
df.loc[0, 'column']        # Single cell by label
df.iloc[0, 1]              # Single cell by position
df.at[0, 'column']         # Fast scalar access
df.iat[0, 1]               # Fast scalar access by position
```

### Data Cleaning
```python
# Handle missing values
df.dropna()                      # Remove rows with any NaN
df.dropna(axis=1)               # Remove columns with any NaN
df.dropna(subset=['col1'])      # Remove rows with NaN in specific column
df.fillna(0)                    # Fill NaN with 0
df.fillna(method='ffill')       # Forward fill
df.fillna(method='bfill')       # Backward fill
df.fillna(df.mean())            # Fill with mean

# Remove duplicates
df.drop_duplicates()
df.drop_duplicates(subset=['col1'])
df.drop_duplicates(keep='last')

# Replace values
df.replace('old_value', 'new_value')
df.replace({'col1': {'old': 'new'}})

# Change data types
df['column'] = df['column'].astype('int64')
df['date_col'] = pd.to_datetime(df['date_col'])
```

### Data Transformation
```python
# Add/modify columns
df['new_column'] = df['col1'] + df['col2']
df['calculated'] = df['col1'] * 2

# Apply functions
df['col1'].apply(lambda x: x**2)
df.apply(lambda row: row['col1'] + row['col2'], axis=1)

# String operations
df['text_col'].str.lower()
df['text_col'].str.contains('pattern')
df['text_col'].str.replace('old', 'new')
df['text_col'].str.split(',')

# Rename columns
df.rename(columns={'old_name': 'new_name'})
df.columns = ['new_col1', 'new_col2', 'new_col3']

# Reorder columns
df = df[['col3', 'col1', 'col2']]
```

## Advanced Usage

### GroupBy Operations
```python
# Basic grouping
grouped = df.groupby('category')
grouped.sum()
grouped.mean()
grouped.count()
grouped.agg(['sum', 'mean', 'count'])

# Multiple grouping columns
df.groupby(['cat1', 'cat2']).sum()

# Custom aggregations
df.groupby('category').agg({
    'sales': 'sum',
    'quantity': 'mean',
    'price': ['min', 'max']
})

# Transform (same shape as original)
df['running_total'] = df.groupby('category')['sales'].cumsum()
df['group_mean'] = df.groupby('category')['sales'].transform('mean')

# Filter groups
df.groupby('category').filter(lambda x: len(x) > 5)
```

### Merging and Joining
```python
# Merge DataFrames
result = pd.merge(df1, df2, on='key')
result = pd.merge(df1, df2, left_on='left_key', right_on='right_key')
result = pd.merge(df1, df2, on='key', how='left')  # left, right, outer, inner

# Join on index
result = df1.join(df2, on='key')

# Concatenate
result = pd.concat([df1, df2])                    # Vertically
result = pd.concat([df1, df2], axis=1)            # Horizontally
result = pd.concat([df1, df2], ignore_index=True) # Reset index
```

### Pivot Tables and Reshaping
```python
# Pivot table
pivot = df.pivot_table(
    values='sales', 
    index='date', 
    columns='category', 
    aggfunc='sum'
)

# Pivot (reshape)
pivot = df.pivot(index='date', columns='category', values='sales')

# Melt (unpivot)
melted = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])

# Stack/Unstack
stacked = df.stack()
unstacked = df.unstack()
```

### Time Series Operations
```python
# Create datetime index
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# Resample time series
df.resample('D').mean()    # Daily average
df.resample('M').sum()     # Monthly sum
df.resample('Q').last()    # Quarterly last value

# Rolling windows
df['rolling_mean'] = df['value'].rolling(window=7).mean()
df['expanding_sum'] = df['value'].expanding().sum()

# Shift data
df['lagged'] = df['value'].shift(1)     # Lag by 1 period
df['lead'] = df['value'].shift(-1)      # Lead by 1 period

# Date operations
df['year'] = df.index.year
df['month'] = df.index.month
df['dayofweek'] = df.index.dayofweek
```

## Tips and Tricks

### Performance Optimization
```python
# Use vectorized operations
df['result'] = df['col1'] * df['col2']  # Instead of loops

# Use query for filtering
df.query('col1 > 5 and col2 < 10')

# Use categorical for repeated strings
df['category'] = df['category'].astype('category')

# Use copy() when needed
df_copy = df.copy()  # Deep copy
df_view = df.copy(deep=False)  # Shallow copy

# Memory usage
df.memory_usage(deep=True)
df.info(memory_usage='deep')
```

### Data Quality Checks
```python
# Check data types
df.dtypes
df.select_dtypes(include=[np.number])  # Select numeric columns

# Validate data
assert df['age'].min() >= 0, "Age cannot be negative"
assert df['email'].str.contains('@').all(), "Invalid email format"

# Profile data
df.describe(include='all')
df.isnull().sum()
df.duplicated().sum()
```

### Working with Large Datasets
```python
# Read in chunks
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    process_chunk(chunk)

# Use efficient data types
df = pd.read_csv('file.csv', dtype={
    'id': 'int32',
    'category': 'category',
    'amount': 'float32'
})

# Use low_memory option
df = pd.read_csv('file.csv', low_memory=False)
```

### Debugging and Exploration
```python
# Display options for better viewing
with pd.option_context('display.max_rows', None):
    print(df)

# Debug chain operations
result = (df
    .groupby('category')
    .agg({'sales': 'sum'})
    .reset_index()
    .sort_values('sales', ascending=False)
)

# Use pipe for custom functions
def custom_transform(df):
    return df[df['value'] > df['value'].mean()]

result = df.pipe(custom_transform)
```

Created: 2025-06-02 (Updated with comprehensive pandas usage guide)
